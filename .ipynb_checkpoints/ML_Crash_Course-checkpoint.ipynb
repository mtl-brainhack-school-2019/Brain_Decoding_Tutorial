{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Everything you need to know (not really actually) about machine learning, as a non-data scientist\n",
    "\n",
    "This is a short guide to the code and machine learning theory used in my project. \n",
    "\n",
    "Note 1: It is strongly recommended that you read this document in order, as ML concepts are introduced and explained as we go along.\n",
    "\n",
    "Note 2: This introduction does not delve too deep into the mathematical theory of ML because this crash course is only meant to cover enough material to follow the basic models in the `Brain_Decoding.ipynb` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## The Basics\n",
    "\n",
    "### So, what the heck is machine learning? \n",
    "\n",
    "First, let's begin with artificial intelligence. Its definition is quite broad, but one could say that it describes machines that can complete complex tasks usually done by humans. These tasks include problems that seem trivial to humans, like image classification, but which have been historically nearly impossible for computers to crack. However, the tasks tackled by artificial intelligence have not been limited by human performance; in fact, many artificial algorithms *outperform* humans. For example, Google's Alpha Go computer program recently beat the best Go player in the world. (todo: add source)\n",
    "\n",
    "<img src=\"https://i.postimg.cc/bNTvMtkM/ml.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "source: [Sumo Logic](https://www.sumologic.com/blog/machine-learning-deep-learning/)\n",
    "\n",
    "Anyways, machine learning is a specific field within artificial intelligence. Traditionally, programming consisted of writing a set on instructions for the computer to run, and to correct these instructions as needed. The computer will compute these instructions and return a calculation, an output. However, this has severe limitations in that we can only use computers to solve problems where we already \"knew what to do\". The computer is used purely to compute our calculations and so the computer is limited by what rules we give it. With machine learning, we give the computer *data* instead of *instructions*, and the computer's task is to figure out the *instructions*. In a sentence, machine learning describes models that correct themselves using applied math rather than through manual instruction. \n",
    "\n",
    "[![trad.png](https://i.postimg.cc/vH4Lhbpp/trad.png)](https://postimg.cc/RNzHV5CR)\n",
    "\n",
    "source: [Data Science Central](https://www.datasciencecentral.com/profiles/blogs/traditional-programming-versus-machine-learning-in-one-picture)\n",
    "\n",
    "***\n",
    "\n",
    "### A quick note on data\n",
    "\n",
    "In general, data is stored as *samples*. A sample is a data point from your data set. For this tutorial, each sample is made of a *label* and *features*. A feature is a property of your data. A label is a characteristic of your data you want to predict. \n",
    "\n",
    "Here is an example to tie these terms together:\n",
    "\n",
    "Imagine you have a dataset of NBA players:\n",
    "\n",
    "|         |             | NBA Players of the 2018-2019 Season  |              |            |\n",
    "|---------|-------------|--------------------------------------|--------------|------------|\n",
    "| Players | Points/Game | Rebounds/Game                        | Assists/Game | Salary     |\n",
    "| Harden  | 36.1        | 6.6                                  | 7.5          | 30 421 854 |\n",
    "| Curry   | 27.3        | 5.3                                  | 5.2          | 37 457 154 |\n",
    "| Durant  | 26.0        | 6.4                                  | 5.9          | 30 000 000 |\n",
    "| James   | 27.4        | 8.5                                  | 8.3          | 35 654 150 |\n",
    "| Leonard | 26.6        | 7.3                                  | 3.3          | ?          |\n",
    "\n",
    "Each player is a sample, their features are their per-game statistics (PPG, RPG, APG) and their salaries are the labels. An example of a machine learning problem would be to try to predict their salary based off their statistics. Our model would learn *how* PPG, RPG and APG affect salary by using the known samples: Harden, Curry, Durrant and James. Then, we can use the model to predict the salary of players that we don't know, like Leonard.\n",
    "\n",
    "***\n",
    "\n",
    "This tutorial deals exclusively with *supervised* learning. This means that when we train our model, we give it the labels so that it can learn the connections between features and label. In *unsupervised* learning tasks, we do *not* give the labels during training. It's a whole other topic which is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Okay, but what does it *look* like?\n",
    "\n",
    "Generally, machine learning models have the same steps:\n",
    "\n",
    "1. **Split the data**: As I just mentioned, machine learning models \"learn\" by going through loads of data. So, the first step is to split the data into a training and test set (and potentially with cross-validation, which we will get to later). Unsurprisingly, the training set is used to train the model and the testing set is used to test the model's accuracy. The reason we keep these separate is because the test results from the model on data it has already seen is useless. Imagine you were writing a test. You then give your friends the answers to the test and test them. Their near perfect scores are not representative of how well they know the material; they already knew what the answers were! Back to machine learning, testing your model on data it has already seen is not indicative of whether or not your model learned to draw meaningful connections between features and labels. \n",
    "\n",
    "2. **Train the model**: The second step is to train the model on the training set data. Before training, we will define some metrics and set some hyper-parameters through which we will adjust and fine-tune our model. For now, a *hyper-parameter* is a setting within the model that dictates how the model makes predictions and a *metric* is a measure of how \"good\" the model is doing in its predictions. Then, we will use the model to predict the labels, see in what way it is incorrect, and adjust the model's hyper-parameters in order to prevent these errors in the future. If this isn't clear now, hopefully it will be once we jump into the inner workings of some common models.\n",
    "\n",
    "3. **Test the model**: The last step is testing the model. Generally, this means using the model to predict the labels of the test set data, and seeing how accurate its predictions are. \n",
    "\n",
    "***\n",
    "\n",
    "Roughly in order of complexity, the models explored in this tutorial are:\n",
    "\n",
    "- (Supervised) k-Nearest Neighbours (kNN)\n",
    "- Logistic Regression\n",
    "- Support Vector Machines (SVM)\n",
    "- Convolutional Neural Network (CNN)\n",
    "\n",
    "Note: There are typically many, *many* variants of each machine learning model. I have chosen the most basic versions but I will mention the extra bells and whistles you can explore on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### k-Nearest Neighbours (kNN)\n",
    "\n",
    "**The idea**: <br>\n",
    "Say, for example, I have two sets of points, green and blue, situated on a Cartesian plane. \n",
    "\n",
    "\n",
    "<img src=\"https://i.postimg.cc/QMGhdgL0/Screen-Shot-2019-08-23-at-2-02-04-PM.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "Their coordinates are their features, and their colour is their label. The idea of a kNN is that if I have a point whose colour I do not know, I can place this point onto the same coordinate system, and assume that this point belongs to the same group as the points closest to it on the coordinate space. A point (sample) is given the same colour (label) as other points whose coordinates (features) are closest to it. For example, if I was considering the 3 closest points, then the unknown point in the visual above would be classified as green because there are 2 green points and 1 blue point. \n",
    "\n",
    "Note: Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Other models we will explore will be generalized models.\n",
    "\n",
    "**The 'k' value**: <br>\n",
    "The 'k' in \"k-Nearest Neighbours\" defines how many points we want to consider in the classification of our point. For example, if we define `k` to be 3, we look at the 3 closest points. In the diagram, that would be 2 green points and 1 blue point. So, we would conclude our unknown point is green. However, if we define our `k` to be 5, we would look at the 5 closest points and conclude that the point is blue. \n",
    "\n",
    "`k` is an example of a *hyper-parameter*. A hyper-parameter is a setting that we must decide before the training and testing of our model. This is a first look at how selecting hyper-parameters can strongly influence our model.\n",
    "\n",
    "Choosing the optimal `k` value is strongly dependant on your data. In a classification task, we assume that a dataset can be classified in two (or more) categories. However, almost all real world data is affected by noise, meaning that the classifications of points is not completely distinct (ie, your data is not perfectly separated). \n",
    "\n",
    "To show what that means, I've created a kNN classifier analyzing the famous iris data set. (todo: fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fix the dash application (does not work)\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools # installed with conda\n",
    "from plotly import subplots\n",
    "\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import dash # installed with conda\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "from jupyter_plotly_dash import JupyterDash # installed with pip\n",
    "\n",
    "app = JupyterDash('myapp')\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H4('Select K'),\n",
    "    dcc.Slider(\n",
    "        id='my-slider',\n",
    "        min=1,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        value=7,\n",
    "        marks={i: '{}'.format(i) for i in range(21)}\n",
    "        \n",
    "    ),\n",
    "    html.H4('Select Algorithm'),\n",
    "    dcc.Dropdown(\n",
    "        id='my-dropdown',\n",
    "        options=[\n",
    "            {'label': 'Auto', 'value': 'auto'},\n",
    "            {'label': 'Ball tree', 'value': 'ball_tree'},\n",
    "            {'label': 'KD Tree', 'value': 'kd_tree'},\n",
    "            {'label': 'Brute Force', 'value': 'brute'},\n",
    "        ],\n",
    "        value='ball_tree',\n",
    "        style={'marginTop': '1.5em'}\n",
    "    ),\n",
    "    html.Div(id='slider-output-container'),\n",
    "    dcc.Graph(id='graph-with-slider')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('graph-with-slider', 'figure'),\n",
    "    [dash.dependencies.Input('my-slider', 'value'),\n",
    "     dash.dependencies.Input('my-dropdown', 'value')\n",
    "    ])\n",
    "\n",
    "\n",
    "def update_output(value,value2):\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, [0, 2]]\n",
    "    y = iris.target\n",
    "    clf2 = KNeighborsClassifier(n_neighbors=value,algorithm=value2)\n",
    "\n",
    "    clf2.fit(X, y)\n",
    "    \n",
    "    titles =  (['KNN k= '+ str(value) + ' Algorithm: ' + value2])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "    y_ = np.arange(y_min, y_max, 0.1)\n",
    "\n",
    "    Zfirst = clf2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Zfirst.reshape(xx.shape)\n",
    "\n",
    "    trace1 = go.Contour(x=xx[0], y=y_, \n",
    "                        z=Z,\n",
    "                        opacity=0.5,\n",
    "                        showscale=False\n",
    "                        )\n",
    "\n",
    "    trace2 = go.Scatter(x=X[:, 0], y=X[:, 1], \n",
    "                        showlegend=False,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                        size=16,\n",
    "                        opacity = 0.9,    \n",
    "                        color = y, #set color equal to a variable\n",
    "                        colorscale=['cyan','purple','orange'],\n",
    "                        showscale=False\n",
    "                    )\n",
    "\n",
    "                       )\n",
    "\n",
    "    fig = subplots.make_subplots(rows=1, cols=1,\n",
    "                          print_grid=True,\n",
    "                          subplot_titles=titles\n",
    "                         )\n",
    "\n",
    "    fig.append_trace(trace1,1,1)\n",
    "    fig.append_trace(trace2,1,1)\n",
    "\n",
    "    return fig \n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The smaller the `k` value, the more accurate the predictions will be, because you will be able to correctly classify the points that are \"unique\" (like those that are blue but that are situated among red points; they are the reason why the classification is not very clear). However, the more sensitive to noise the classifier will be. The classifier would be more accurate to *the dataset in question* and not datasets in general. This means that your classifier will be prone to *overfitting* because not all datasets will contain those \"unique\" points that your classifier has adapted to. \n",
    "\n",
    "A greater `k` value will be less accurate to the dataset in question (as it will incorrectly classify the \"unique\" points) but it will be more generalized, meaning that it will be more applicable to unknown datasets. By considering more points from the dataset for each prediction, its predictions are averaged out over a greater number, so its predictions are more general, because it is less affected by noise (the \"unique\" points). \n",
    "\n",
    "Something to keep in mind is that the higher our `k` value, the more computationally expensive our model is. That's because the more points we are using to compare to our unknown point, the more computations must be completed. \n",
    "\n",
    "There is another neighbor classifier which is implemented through the [RadiusNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) class. It selects points for classification through a fixed radius `r` around the unknown point. We will focus on the `kNeighborsClassifier` but feel free to explore on your own.\n",
    "\n",
    "These neighbors-based classifiers are not like typical machine learning models. They do not actually model the data; rather, they keep all the training data in memory and search this collection when asked to classify unknown data. Thus, they are called *non-generalizing* machine learning methods.\n",
    "\n",
    "**The notion of 'distance'**: <br>\n",
    "The \"distance\" that is computed for kNNs has several forms, because there are different ways to define it. The default metric in sk-learn is the 'minkowski' distance, defined as `sum(|x - y|^p)^(1/p)`, which is stored in the `metric` argument for the kNN class. This metric requires you also set the `p` parameter. The default value is `2`, and so it is equivalent to classic euclidean distance. Other choices for the distance metric can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n",
    "\n",
    "***\n",
    "\n",
    "Another part of the `KNeighborsClassifier` class is `metric_params`, which takes in keyword arguments for specific metric functions, if needed. \n",
    "\n",
    "**A note on dimensionality**: <br>\n",
    "Practically speaking, we often don't restrict ourselves to data that can only be described in 2-D. Real world data is often characterized by many more features than just two. Although hard to visualize, k-Nearest Neighbours can be applied to data of much higher dimensions and the math is still valid. In this tutorial, each sample is a brain image, and the *features* of each sample is the activation values of each voxel. \n",
    "\n",
    "**The notion of 'weights'**: <br>\n",
    "When implementing a kNN, another hyper-parameter to consider is the weight function used in prediction. You must choose how strongly you want your points to be considered. In sk-learn, the default argument is `'uniform'`, meaning that all points that are being used for the classification are valued equally. Conversely, the `distance` argument will weigh points by the inverse of their distance. This means that points further away will have lesser influence on the classification of the point. \n",
    "\n",
    "**The algorithm: how the model searches its memory, and orders of complexity**: <br>\n",
    "To classify a test set, the kNN has to compute the distances between its training and test sets. For each test point, it must also find which training points are closest. This turns out to be quite a lot of computation, and the computational workload of a problem is often described in [*Big-O notation*](https://en.wikipedia.org/wiki/Big_O_notation#Orders_of_common_functions). Big-O notation is a mathematical notation that describes how well an algorithm can scale to larger datasets.\n",
    "\n",
    "When we are dealing with small datasets, the choice of algorithm and the strength of processor is often insignificant. However, when we deal with larger datasets, the amount of computation can be extremely demanding. To increase the speed at which we process data, we can keep improving our hardware (think [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law)), but another (perhaps more) important way is to think of better searching algorithms. \n",
    "\n",
    "We can explore this idea by going through a naive *brute force* searching algorithm for kNNs. Brute-force algorithms do not take advantage of the clever tricks mathematicians have found to speed up the process; they try to exhaust all possibilities in order to find the solution. \n",
    "\n",
    "To start, finding the distance (let's assume Euclidean) between one sample to another is described by `O(d)`. The equation for Euclidean distance between sample `a` and sample `b` in `d`-space is:\n",
    "\n",
    "$D(a,b) = \\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+...+(a_d-b_d)^2}$\n",
    "\n",
    "All that means is that for every dimension (feature) in a sample, we have *one* computation to do. If we were in 1-D space, we would only need to compute:\n",
    "\n",
    "$D(a,b) = \\sqrt{(a_1-b_1)}$\n",
    "\n",
    "If we were in 2-D space, we would need to compute: \n",
    "\n",
    "$D(a,b) = \\sqrt{(a_1-b_1)^2+(a_2-b_2)^2}$\n",
    "\n",
    "and so on.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/dtSc5rbR/O-d-vsd.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "Now, in order to find the distance between one sample and all the other samples, we must do this `n` times, where `n` is the total number of samples. So, our computation time is now described by `O(nd)`. Finally, we want to do this for every sample in our data, so we must do this `n` times, once for each sample. So, our computation is described by <code>O(dn<sup>2</sup>)</code>. \n",
    "\n",
    "![complexity_gif](https://media.giphy.com/media/h1zI00j8WD9aoVUgEv/giphy.gif)\n",
    "\n",
    "This distance computation doesn't even factor in the time it would take to search for the `k` closest neighbors! Continuing from our computations from above, we would then have to sort the samples in order to find the `k` closest ones. A brute force sorting algorithm is [selection sort] (https://en.wikipedia.org/wiki/Selection_sort), where you iterate through your list of samples one by one until you find the smallest distance and place it at the beginning of your list. You then search the rest of the list for the next smallest distance, and so on. \n",
    "\n",
    "In sk-learn, if you want to use a brute force algorithm, you must specify the `'brute'` option in in the `'algorithm'` argument of the `KNeighborsClassifier()` class.\n",
    "\n",
    "Admittedly, this way of doing it ends up with a lot of redundencies, but that is common to brute force algorithms. They aren't known for their efficiency, they are known for their exhaustiveness. This is why we develop clever algorithms!\n",
    "\n",
    "For example, several data structures have been created in order to avoid the computational redundancy of brute force algorithms. In sk-learn, this includes the [K-D tree](https://en.wikipedia.org/wiki/K-d_tree#Nearest_neighbour_search) data structure and the [Ball Tree](https://en.wikipedia.org/wiki/Ball_tree) data structure.\n",
    "\n",
    "The advanced theory of these data structures are beyond the scope of this tutorial, but for more information on these tricks, see the [sk-learn documentation](https://scikit-learn.org/stable/modules/neighbors.html) as well as the *Further Reading* section at the end of this Notebook.\n",
    "\n",
    "***\n",
    "\n",
    "The `leaf_size` argument specifies the leaf size passed to BallTree or KDTree. This is a hyper-parameter that affects the memory required to store the tree, the speed of computation, etc., and is outside the scope of this tutorial. \n",
    "\n",
    "***\n",
    "\n",
    "And that's it! This is all the material related to the k-Nearest Neighbor algorithm in sk-learn! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Logistic Regression: Coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Further reading:\n",
    "\n",
    "brute force in knn:\n",
    "https://web.cs.ucdavis.edu/~amenta/pubs/bfknn.pdf\n",
    "\n",
    "k-d trees:\n",
    "https://www.quora.com/How-does-a-k-d-tree-find-the-K-nearest-neighbors\n",
    "\n",
    "complexity in knns:\n",
    "https://stanford.edu/~rezab/classes/cme323/S16/projects_reports/neeb_kurrus.pdf\n",
    "http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Sources:\n",
    "\n",
    "sklearn kNN theory: https://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "kNN sklearn class: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
